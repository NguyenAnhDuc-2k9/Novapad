PROMPT PER CODEX: Metodo per scaricare articoli da New Scientist RSS Feed

=== OBIETTIVO ===
Creare un'applicazione Rust che scarica articoli completi dal feed RSS di New Scientist,
estraendo il contenuto testuale dall'HTML e salvandolo in file .txt.

=== DIPENDENZE CARGO.TOML ===
```toml
[dependencies]
reqwest = { version = "0.12", features = ["rustls-tls", "cookies"] }
tokio = { version = "1", features = ["full"] }
rss = "2.0"
scraper = "0.20"
```

=== STRATEGIA ANTI-BOT ===
Per bypassare le protezioni anti-bot di New Scientist, simulare un browser reale:

1. Headers HTTP realistici:
   - User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:136.0) Gecko/20100101 Firefox/136.0
   - Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8
   - Accept-Language: en-US,en;q=0.5
   - DNT: 1
   - Connection: keep-alive
   - Upgrade-Insecure-Requests: 1
   - Sec-Fetch-Dest: document
   - Sec-Fetch-Mode: navigate
   - Sec-Fetch-Site: cross-site
   - Sec-Fetch-User: ?1
   - Referer: https://www.newscientist.com/

2. Cookie store abilitato per mantenere la sessione

3. Gestione redirect automatica (max 10)

4. Timeout di 30 secondi per richiesta

5. Rate limiting: 1 secondo tra le richieste per essere "gentili" con il server

=== PROCESSO DI ESTRAZIONE ===

STEP 1: Fetch RSS Feed
- URL: https://www.newscientist.com/feed/home
- Parsing con crate `rss`
- Il feed contiene ~100 articoli con titolo e link

STEP 2: Per ogni articolo nel feed
- Fare richiesta HTTP GET con tutti gli headers sopra
- Aggiungere header Referer per sembrare una navigazione interna
- Ricevere HTML completo (circa 433KB per articolo)

STEP 3: Parsing HTML con `scraper`
- Usare selettori CSS multipli in ordine di priorità:
  1. "article"
  2. ".article-content"
  3. ".article__body"
  4. ".entry-content"
  5. "main article"
  6. "[data-test='article-body']"
  7. ".post-content"
  8. "main"

- Il selettore "article" funziona per New Scientist
- Estrarre tutto il testo con .text().collect()

STEP 4: Pulizia contenuto
- Rimuovere whitespace eccessivo
- Split su whitespace e join con spazio singolo
- Validare lunghezza minima (>200 caratteri)

STEP 5: Salvataggio file
- Creare directory "articles/" se non esiste
- Nome file sicuro: rimuovere caratteri speciali, limitare lunghezza
- Formato: {numero}_{titolo_safe}.txt
- Contenuto: Titolo + URL + Contenuto estratto

=== SELETTORE CSS VINCENTE ===
Per New Scientist: "article"
Questo selettore cattura tutto l'articolo inclusi titolo, autore, data e corpo del testo.
Estrae circa 6.000-17.000 caratteri di contenuto pulito per articolo.

=== RISULTATI ATTESI ===
- Tasso di successo: 100% su New Scientist
- Dimensione contenuto: 5.000-20.000 caratteri per articolo
- Tempo: ~1-2 secondi per articolo (con rate limiting)
- Formato output: Plain text leggibile

=== CODICE CHIAVE ===

```rust
// Client HTTP con headers realistici
let mut headers = HeaderMap::new();
headers.insert(USER_AGENT, HeaderValue::from_static("Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:136.0) Gecko/20100101 Firefox/136.0"));
headers.insert(ACCEPT, HeaderValue::from_static("text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8"));
headers.insert(ACCEPT_LANGUAGE, HeaderValue::from_static("en-US,en;q=0.5"));
headers.insert(DNT, HeaderValue::from_static("1"));
headers.insert(CONNECTION, HeaderValue::from_static("keep-alive"));
headers.insert(UPGRADE_INSECURE_REQUESTS, HeaderValue::from_static("1"));
headers.insert("Sec-Fetch-Dest", HeaderValue::from_static("document"));
headers.insert("Sec-Fetch-Mode", HeaderValue::from_static("navigate"));
headers.insert("Sec-Fetch-Site", HeaderValue::from_static("cross-site"));
headers.insert("Sec-Fetch-User", HeaderValue::from_static("?1"));

let client = reqwest::Client::builder()
    .default_headers(headers)
    .cookie_store(true)
    .redirect(reqwest::redirect::Policy::limited(10))
    .timeout(std::time::Duration::from_secs(30))
    .build()?;

// Fetch articolo con Referer
let resp = client
    .get(article_url)
    .header(REFERER, "https://www.newscientist.com/")
    .send()
    .await?;

let html = resp.text().await?;
let document = Html::parse_document(&html);

// Estrazione contenuto
let selector = Selector::parse("article")?;
if let Some(element) = document.select(&selector).next() {
    let text = element.text().collect::<Vec<_>>().join(" ");
    let cleaned = text.split_whitespace().collect::<Vec<_>>().join(" ");
    // cleaned contiene il contenuto pulito dell'articolo
}

// Rate limiting
tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;
```

=== PERCHÉ FUNZIONA ===
1. Headers identici a browser reale (Firefox)
2. Cookie store mantiene la sessione
3. Referer fa sembrare navigazione interna al sito
4. Sec-Fetch-* headers indicano richiesta legittima da browser
5. Rate limiting evita di apparire come bot aggressivo
6. Selettore CSS "article" cattura contenuto semantico HTML5

=== SITI COMPATIBILI ===
Questa tecnica funziona per:
- New Scientist ✅
- WordPress blogs ✅
- Medium ✅
- The Guardian ✅
- BBC ✅
- Blog personali ✅

NON funziona per:
- NYTimes (usa Cloudflare + TLS fingerprinting)
- Siti con paywall JavaScript
- Siti con contenuto caricato dinamicamente via JS

=== ALTERNATIVE PER SITI PROTETTI ===
Per siti come NYTimes serve:
- TLS fingerprinting (crate `wreq` con BoringSSL)
- Headless browser (Puppeteer/Selenium)
- API ufficiali quando disponibili
